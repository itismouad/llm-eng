{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eo_QP1ITFfX2"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Pre-Training of GPT-Style Model\n",
        "\n",
        "In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n",
        "\n",
        "The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n",
        "\n",
        "> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."
      ],
      "metadata": {
        "id": "UWiGVj6njoDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Selection\n",
        "\n",
        "For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n",
        "\n",
        "You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n",
        "\n",
        "> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n",
        "\n",
        "Let's start by grabbing our source repository for the day!"
      ],
      "metadata": {
        "id": "eHi04aEnkKEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRsEQZy6tgc",
        "outputId": "acd61e3a-6d30-4c5f-bb32-7def2fdb8a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 649, done.\u001b[K\n",
            "remote: Total 649 (delta 0), reused 0 (delta 0), pack-reused 649\u001b[K\n",
            "Receiving objects: 100% (649/649), 935.48 KiB | 17.65 MiB/s, done.\n",
            "Resolving deltas: 100% (373/373), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll need to grab some dependencies.\n",
        "\n",
        "`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."
      ],
      "metadata": {
        "id": "6l4CqoEDl7ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken requests cohere openai -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_gepPv1Qdj_",
        "outputId": "6d37e3d8-eb76-4589-e51d-c6a9a5a4c9c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.2/220.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first - let's download our dataset!\n",
        "\n",
        "We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set. We want ~90% of our data to be training, and ~10% to be validation."
      ],
      "metadata": {
        "id": "70hSjXmZmCt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "current_path = \"/data/shakespeare\"\n",
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not os.path.exists(current_path):\n",
        "    os.makedirs(current_path)\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "train_data = data[:round(n*0.9)]\n",
        "val_data = data[round(n*0.9):]"
      ],
      "metadata": {
        "id": "T7qRWArUNiZ5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpeZ8cetnjvp",
        "outputId": "9e4e1a96-f410-447a-9426-d4e507669ba0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003855 111539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."
      ],
      "metadata": {
        "id": "wU9BG2CymU-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFnrwKpQPsYh",
        "outputId": "ee01c705-11a6-48a4-c0e7-25c22caa3826"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n",
        "\n",
        "Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmWXE5ctma9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is BPE?\n",
        "\n",
        "First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n",
        "\n",
        "The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n",
        "\n",
        "Let's take the following text and break it apart into its word components.\n",
        "\n",
        "\n",
        "```\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "```\n",
        "\n",
        "A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."
      ],
      "metadata": {
        "id": "GLecDiHbogvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "\"\"\"\n",
        "\n",
        "naive_word_list = input_text.split()"
      ],
      "metadata": {
        "id": "m34NDAGCpiz6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"starting by splitting by space:\")\n",
        "naive_word_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOXKWKRbnxBX",
        "outputId": "4db38e5b-34b4-40f7-8c5f-959b0d11f09c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting by splitting by space:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['After',\n",
              " 'pre-tokenization,',\n",
              " 'a',\n",
              " 'set',\n",
              " 'of',\n",
              " 'unique',\n",
              " 'words',\n",
              " 'has',\n",
              " 'been',\n",
              " 'created']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can count our words and get their frequency."
      ],
      "metadata": {
        "id": "hR8k-2bopqjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "vocab_and_frequencies = defaultdict(int)\n",
        "\n",
        "for word in naive_word_list:\n",
        "  vocab_and_frequencies[\" \".join(list(word))] += 1\n",
        "\n",
        "list(vocab_and_frequencies.keys())[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_201bSQpvqD",
        "outputId": "cc0be918-2b27-4456-c7ab-5bae4037c333"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A f t e r', 'p r e - t o k e n i z a t i o n ,', 'a', 's e t', 'o f']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"we get the frequency for each of the initial tokens :\")\n",
        "sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtZ-FV_gn-M0",
        "outputId": "dd3fe64e-4029-4af7-c436-98c7e88072f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we get the frequency for each of the initial tokens :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."
      ],
      "metadata": {
        "id": "NckufSxxp-w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List, Set\n",
        "\n",
        "def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n",
        "  vocab = set()\n",
        "\n",
        "  for word in current_vocab.keys():\n",
        "    for subword in word.split():\n",
        "      vocab.add(subword)\n",
        "\n",
        "  return vocab, len(vocab)"
      ],
      "metadata": {
        "id": "BNcjzjDvvKjp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, vocab_length = find_vocabulary_size(vocab_and_frequencies)"
      ],
      "metadata": {
        "id": "pf3kCf-WvdBL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4Wpv0SSpI1F",
        "outputId": "f24f02bd-4211-4cca-b17c-6aeb2777e5d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'o', ',', 'e', 's', 'w', 'a', 'u', 'p', 'l', 't', 'n', 'r', 'N', 'I', '-', 'P', 'h', 'm', 'B', 'b', 'v', 'f', 'd', 'c', 'E', 'i', 'g', 'z', 'x', 'k', '.', 'q', 'A', 'y'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are ~34 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."
      ],
      "metadata": {
        "id": "VoMq7GhKqf7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."
      ],
      "metadata": {
        "id": "OGxrHYmftDTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  pairs = {}\n",
        "\n",
        "  for word, frequency in current_vocab.items():\n",
        "    symbols = word.split()\n",
        "\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pair = (symbols[i], symbols[i + 1])\n",
        "      current_frequency = pairs.get(pair, 0)\n",
        "      pairs[pair] = current_frequency + frequency\n",
        "\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "sTwvfTAErQN7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"
      ],
      "metadata": {
        "id": "FudOaKmYv9-y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIJfkk7wFYw",
        "outputId": "26086716-aed5-4380-e65e-69f839edddb6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('t', 'h'), 11),\n",
              " (('i', 'n'), 10),\n",
              " (('r', 'e'), 8),\n",
              " (('h', 'e'), 8),\n",
              " (('a', 't'), 7)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the frequent pairs - we can merge those pairs into a single token.\n",
        "\n",
        "Let's see how this process looks in code."
      ],
      "metadata": {
        "id": "OqORqdzwsZ6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  vocab_out = {}\n",
        "\n",
        "  pattern = re.escape(' '.join(most_common_pair))\n",
        "  replacement = ''.join(most_common_pair)\n",
        "\n",
        "  for word_in in current_vocab:\n",
        "      word_out = re.sub(pattern, replacement, word_in)\n",
        "      vocab_out[word_out] = current_vocab[word_in]\n",
        "\n",
        "  return vocab_out"
      ],
      "metadata": {
        "id": "L7ohHm2kshoY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance"
      ],
      "metadata": {
        "id": "SynPO7LTptVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_OngxKnpmX8",
        "outputId": "5044ea2f-b294-4a35-b1b3-d2f8c2bec8c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('t', 'h'),)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab_and_frequencies = merge_vocab(\n",
        "    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n",
        "    vocab_and_frequencies\n",
        ")"
      ],
      "metadata": {
        "id": "Ab760KKuwzZ6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JZJ3yEIr8wP",
        "outputId": "5882e2cf-5e1a-43aa-c6aa-a829022df0ec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab, new_vocab_length = find_vocabulary_size(new_vocab_and_frequencies)"
      ],
      "metadata": {
        "id": "L0XtvLbpxbSx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After one merge, we can see that `t h` has been converted to `th`!\n",
        "\n",
        "Let's see how that impacted our vocabulary."
      ],
      "metadata": {
        "id": "9DPkBzj2u-me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO_xegCtxjQf",
        "outputId": "9b2611bd-4f7d-4d82-bdd5-a33f4108ee58"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n",
        "\n",
        "In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"
      ],
      "metadata": {
        "id": "o3M13D60xzZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Our Tokenizer\n",
        "\n",
        "Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n",
        "\n",
        "Let's walk through the steps we'll take:\n",
        "\n",
        "1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n",
        "\n",
        "  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n",
        "  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n",
        "\n",
        "2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n",
        "\n",
        "  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n",
        "\n",
        "3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n",
        "\n",
        "  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n",
        "  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"
      ],
      "metadata": {
        "id": "BePYCbHly02H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFD, Sequence\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_tokwn=\"[UNK]\"))\n",
        "tokenizer.normalizer = Sequence([\n",
        "    NFD()\n",
        "    ])\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "OrztE09OPosB"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n",
        "\n",
        "Let's use the following:\n",
        "\n",
        "- `\"<s>\"`    : bos_token - beginning of sequence token\n",
        "- `\"</s>\"`   : eos_token - end of sequence token\n",
        "- `\"<pad>\"`  : padding_token - token used to pad sequences\n",
        "- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n",
        "- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n",
        "\n",
        "We're also going to set a target vocabulary of 50,000 tokens."
      ],
      "metadata": {
        "id": "dDqkNNdM1KsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(\n",
        "    vocab_size=50000,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\n",
        "      \"<s>\",\n",
        "      \"<pad>\",\n",
        "      \"</s>\",\n",
        "      \"<unk>\",\n",
        "      \"<mask>\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "x9iQVhN3P3RN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nothing left to do but point it at our data-source and let it train!\n",
        "\n",
        "We'll use the `.train()` method to accomplish this task.\n",
        "\n",
        "> NOTE: Pay attention to the desired inputs of the `.train()` method.\n",
        "\n",
        "- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"
      ],
      "metadata": {
        "id": "yQ8X9vZe2Fyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(\n",
        "    files=[input_file_path],\n",
        "    trainer=trainer\n",
        ")"
      ],
      "metadata": {
        "id": "LinLHotSP7gv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"
      ],
      "metadata": {
        "id": "V2JNYiqB2qKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/tokenizer'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.model.save(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk6QjDGHQy2K",
        "outputId": "c0558d07-2378-4069-b63c-7fb6c332fa6d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOlbggdRFrN",
        "outputId": "117ccd7d-18e6-4210-9311-44600785ff6c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
      ],
      "metadata": {
        "id": "us1vofdhQ45C"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it tokenizes our inputs!"
      ],
      "metadata": {
        "id": "0-Bnq7lV2xWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""
      ],
      "metadata": {
        "id": "dnYnFa3fTRLf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = tokenizer.tokenize(input_sentence)\n",
        "tokenized_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHY5VufRbBj",
        "outputId": "bba0b20b-af47-414c-859f-3134a3395271"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hark',\n",
              " ',',\n",
              " 'Ġmy',\n",
              " 'Ġname',\n",
              " 'Ġbe',\n",
              " 'ĠRomeo',\n",
              " '!',\n",
              " 'ĠI',\n",
              " 'Ġam',\n",
              " 'Ġbut',\n",
              " 'Ġa',\n",
              " 'Ġbeautiful',\n",
              " 'Ġsummer',\n",
              " \"'s\",\n",
              " 'Ġday',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "encoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZrWzQQlTU41",
        "outputId": "97043c20-0393-410e-cde7-4c00e85ad57b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can decode the sequence back to a text"
      ],
      "metadata": {
        "id": "ZMPKHSHX3KIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=False)\n",
        "decoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oS6lE-NLRnzk",
        "outputId": "38a770d8-6eb2-4320-b891-575fc7c502cf"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing Dataset\n",
        "\n",
        "Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n",
        "\n",
        "We'll simply encode our training and validation data - and then save them in binary files for later!\n",
        "\n",
        "> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."
      ],
      "metadata": {
        "id": "ji3sF-rA21YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = tokenizer.encode(train_data)\n",
        "val_ids = tokenizer.encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "calHML6JPnCU",
        "outputId": "46ec8b57-1475-4f51-8845-0240591213b1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 291,285 tokens\n",
            "val has 34,222 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export to bin files\n",
        "data_path = \"/data/shakespeare/\"\n",
        "\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"
      ],
      "metadata": {
        "id": "nKJ1KqiiPkRh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training The Model\n",
        "\n",
        "Now that we have our tokenized dataset, let's get to training our model!\n",
        "\n",
        "We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n",
        "\n",
        "First, let's literally jump into the `nanoGPT` repository we cloned earlier."
      ],
      "metadata": {
        "id": "c0I3VrRC3XIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUU2jaalUdqm",
        "outputId": "08563731-55e4-4b69-cde6-5408bb23f70c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do some critical imports."
      ],
      "metadata": {
        "id": "13p1e8sa3k0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from the local repo\n",
        "from model import GPTConfig, GPT"
      ],
      "metadata": {
        "id": "weNR37BwUYNg"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-Parameters\n",
        "\n",
        "We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."
      ],
      "metadata": {
        "id": "kY_vWZG-3uM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I/O\n",
        "\n",
        "- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"
      ],
      "metadata": {
        "id": "OykCjVQK5EX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = 'out'"
      ],
      "metadata": {
        "id": "viM3qlWt5PVS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization\n",
        "\n",
        "Since we're training from scratch, we'll use `init_from = 'scratch'`."
      ],
      "metadata": {
        "id": "A5iwwrNL5H4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_from = 'scratch'"
      ],
      "metadata": {
        "id": "OK1z2m3C312T"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval and Logging\n",
        "\n",
        "- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n",
        "- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n",
        "- `eval_iters` - this is how *many* iterations we want to evaluate for.\n",
        "- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n",
        "- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."
      ],
      "metadata": {
        "id": "2YlolKOj4_dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 250\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "eval_only = False\n",
        "always_save_checkpoint = True"
      ],
      "metadata": {
        "id": "MbFN5Ltq4_mo"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "We can set our dataset here - we'll use the one we created earlier!"
      ],
      "metadata": {
        "id": "a488zaF_4zQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'shakespeare'"
      ],
      "metadata": {
        "id": "_QC7vWXC40Hp"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Typical Hyper-Parameters\n",
        "\n",
        "- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n",
        "- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n",
        "- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."
      ],
      "metadata": {
        "id": "XP9rBgGc426Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 1\n",
        "batch_size = 16\n",
        "block_size = 512 # another synonym for context window"
      ],
      "metadata": {
        "id": "EM_ybLPP43Pd"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture\n",
        "\n",
        "- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n",
        "- `n_head` - this is the number of attention heads in each decoder layer!\n",
        "- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook. A default value of ~`500` should do the trick!\n",
        "- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n",
        "- `bias` - wether or not to use bias inside the LayerNorm/Linear layers."
      ],
      "metadata": {
        "id": "UZ-8bDIY45GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layer = 6\n",
        "n_head = 6 # hence 6 * 6 = 36 total heads\n",
        "n_embd = 516\n",
        "dropout = 0.2\n",
        "bias = False"
      ],
      "metadata": {
        "id": "gMyyDBxB6k4H"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ QUESTION:\n",
        "\n",
        "What condition must be true as it relates to the `n_embd` and `n_head`?"
      ],
      "metadata": {
        "id": "piNHkSaRNDjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ANSWER` : You need to make sure we can nicely divide `n_embd` (size of our inputs) by `n_heads` (count of attention heads per layer) so that we can split our inputs."
      ],
      "metadata": {
        "id": "OVvc7eJ34efd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer Hyper-Parameters\n",
        "\n",
        "Basic Optimizer Hyper-Parameters:\n",
        "\n",
        "- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n",
        "- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n",
        "\n",
        "Learning Rate Decay Settings:\n",
        "\n",
        "- `decay_lr` - set decay flag\n",
        "- `weight_Decay` - how much to decay lr by\n",
        "- `lr_decay_iters` - should be set to ~max_iters.\n",
        "- `min_lr` - the minimum lr, should be ~ lr / 10\n",
        "\n",
        "Clipping and Warmup:\n",
        "\n",
        "- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n",
        "- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n",
        "\n",
        "> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."
      ],
      "metadata": {
        "id": "3NWDTaAz7gwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adamw optimizer\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5_000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "\n",
        "# lr decay settings\n",
        "decay_lr = True\n",
        "weight_decay = 1e-1\n",
        "lr_decay_iters = 5_000 # ~= max_iters per Chinchilla\n",
        "min_lr = 1e-4 # ~= learning_rate/10 per Chinchilla\n",
        "\n",
        "# clipping and warmup\n",
        "grad_clip = 1.0\n",
        "warmup_iters = 100"
      ],
      "metadata": {
        "id": "qe-669jwUptI"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ QUESTION:\n",
        "\n",
        "Given a Learning Rate of `1e-4` and a maximum iteration cap of `10,000`: What should `lr_decay_iters` be, and what should `min_lr` be?"
      ],
      "metadata": {
        "id": "AzHvpMDTNfU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  `lr_decay_iters` should be approx. `max_iters` (\n",
        "-  `min_lr` should be `learning_rate`/10 to use a 10× learning rate decay (see https://arxiv.org/pdf/2203.15556.pdf)"
      ],
      "metadata": {
        "id": "vgJuUmia7GWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."
      ],
      "metadata": {
        "id": "ucldc4mz9yeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend = 'nccl'\n",
        "device = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = True\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "master_process = True\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiGlMOp8Nux",
        "outputId": "5be0fe53-da2f-40cc-d6a2-ecd97780fe1b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 8,192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Settings\n",
        "\n",
        "We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n",
        "\n",
        "Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."
      ],
      "metadata": {
        "id": "eKmdfbye-BNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "yh34QGD6VARU"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n",
        "\n",
        "This block will:\n",
        "\n",
        "1. Set the data path\n",
        "2. Load the dataset we tokenized earlier from the `.bin` we saved\n",
        "3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."
      ],
      "metadata": {
        "id": "gKeNwYaZ-Zoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join('/data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "tOjaPyJpVEgx"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question:\n",
        "\n",
        "What can you tell us about the way the labels are generated?\n",
        "\n",
        "Please produce an example of a single x and y pair."
      ],
      "metadata": {
        "id": "I-tifZVD-9hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = val_data.copy()\n",
        "ix = torch.randint(len(sample_data) - block_size, (batch_size,))\n",
        "x = torch.stack([torch.from_numpy((sample_data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "y = torch.stack([torch.from_numpy((sample_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])"
      ],
      "metadata": {
        "id": "43XYc3Va-AOO"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "x = tokenizer.decode(sample_data[i:i+block_size], clean_up_tokenization_spaces=False)\n",
        "y = tokenizer.decode(sample_data[i+1:i+1+block_size], clean_up_tokenization_spaces=False)"
      ],
      "metadata": {
        "id": "Ga3dsH26BLnn"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "-FvQTExtBvIl",
        "outputId": "325143b1-b7b2-471f-92e6-6065bf09de8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "GREMIO:\n",
            "Good morrow, neighbour Baptista.\n",
            "\n",
            "BAPTISTA:\n",
            "Good morrow, neighbour Gremio.\n",
            "God save you, gentlemen!\n",
            "\n",
            "PETRUCHIO:\n",
            "And you, good sir! Pray, have you not a daughter\n",
            "Call'd Katharina, fair and virtuous?\n",
            "\n",
            "BAPTISTA:\n",
            "I have a daughter, sir, called Katharina.\n",
            "\n",
            "GREMIO:\n",
            "You are too blunt: go to it orderly.\n",
            "\n",
            "PETRUCHIO:\n",
            "You wrong me, Signior Gremio: give me leave.\n",
            "I am a gentleman of Verona, sir,\n",
            "That, hearing of her beauty and her wit,\n",
            "Her affability and bashful modesty,\n",
            "Her wondrous qualities and mild behavior,\n",
            "Am bold to show myself a forward guest\n",
            "Within your house, to make mine eye the witness\n",
            "Of that report which I so oft have heard.\n",
            "And, for an entrance to my entertainment,\n",
            "I do present you with a man of mine,\n",
            "Cunning in music and the mathematics,\n",
            "To instruct her fully in those sciences,\n",
            "Whereof I know she is not ignorant:\n",
            "Accept of him, or else you do me wrong:\n",
            "His name is Licio, born in Mantua.\n",
            "\n",
            "BAPTISTA:\n",
            "You're welcome, sir; and he, for your good sake.\n",
            "But for my daughter Katharina, this I know,\n",
            "She is not for your turn, the more my grief.\n",
            "\n",
            "PETRUCHIO:\n",
            "I see you do not mean to part with her,\n",
            "Or else you like not of my company.\n",
            "\n",
            "BAPTISTA:\n",
            "Mistake me not; I speak but as I find.\n",
            "Whence are you, sir? what may I call your name?\n",
            "\n",
            "PETRUCHIO:\n",
            "Petruchio is my name; Antonio's son,\n",
            "A man well known throughout all Italy.\n",
            "\n",
            "BAPTISTA:\n",
            "I know him well: you are welcome for his sake.\n",
            "\n",
            "GREMIO:\n",
            "Saving your tale, Petruchio, I pray,\n",
            "Let us, that are poor petitioners, speak too:\n",
            "Baccare! you are marvellous forward.\n",
            "\n",
            "PETRUCHIO:\n",
            "O, pardon me, Signior Gremio; I would fain be doing.\n",
            "\n",
            "GREMIO:\n",
            "I doubt it not, sir; but you will curse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "lznAOQh6Bx8A",
        "outputId": "e3e2dd3d-77d0-4aef-8c99-505c5185238f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GREMIO:\n",
            "Good morrow, neighbour Baptista.\n",
            "\n",
            "BAPTISTA:\n",
            "Good morrow, neighbour Gremio.\n",
            "God save you, gentlemen!\n",
            "\n",
            "PETRUCHIO:\n",
            "And you, good sir! Pray, have you not a daughter\n",
            "Call'd Katharina, fair and virtuous?\n",
            "\n",
            "BAPTISTA:\n",
            "I have a daughter, sir, called Katharina.\n",
            "\n",
            "GREMIO:\n",
            "You are too blunt: go to it orderly.\n",
            "\n",
            "PETRUCHIO:\n",
            "You wrong me, Signior Gremio: give me leave.\n",
            "I am a gentleman of Verona, sir,\n",
            "That, hearing of her beauty and her wit,\n",
            "Her affability and bashful modesty,\n",
            "Her wondrous qualities and mild behavior,\n",
            "Am bold to show myself a forward guest\n",
            "Within your house, to make mine eye the witness\n",
            "Of that report which I so oft have heard.\n",
            "And, for an entrance to my entertainment,\n",
            "I do present you with a man of mine,\n",
            "Cunning in music and the mathematics,\n",
            "To instruct her fully in those sciences,\n",
            "Whereof I know she is not ignorant:\n",
            "Accept of him, or else you do me wrong:\n",
            "His name is Licio, born in Mantua.\n",
            "\n",
            "BAPTISTA:\n",
            "You're welcome, sir; and he, for your good sake.\n",
            "But for my daughter Katharina, this I know,\n",
            "She is not for your turn, the more my grief.\n",
            "\n",
            "PETRUCHIO:\n",
            "I see you do not mean to part with her,\n",
            "Or else you like not of my company.\n",
            "\n",
            "BAPTISTA:\n",
            "Mistake me not; I speak but as I find.\n",
            "Whence are you, sir? what may I call your name?\n",
            "\n",
            "PETRUCHIO:\n",
            "Petruchio is my name; Antonio's son,\n",
            "A man well known throughout all Italy.\n",
            "\n",
            "BAPTISTA:\n",
            "I know him well: you are welcome for his sake.\n",
            "\n",
            "GREMIO:\n",
            "Saving your tale, Petruchio, I pray,\n",
            "Let us, that are poor petitioners, speak too:\n",
            "Baccare! you are marvellous forward.\n",
            "\n",
            "PETRUCHIO:\n",
            "O, pardon me, Signior Gremio; I would fain be doing.\n",
            "\n",
            "GREMIO:\n",
            "I doubt it not, sir; but you will curse your\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we added `your` at the end of the sequence"
      ],
      "metadata": {
        "id": "EJHi2zCgCCjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Initialization of Model\n",
        "\n",
        "Here we init our number of iterations as 0, and our best val loss as a very high number."
      ],
      "metadata": {
        "id": "EbDlW-68_atH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9"
      ],
      "metadata": {
        "id": "6hsepdVBVzQU"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain our vocab size from our trained tokenizer."
      ],
      "metadata": {
        "id": "A4Uj9qBI_vXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = tokenizer.vocab_size\n",
        "meta_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m53DcCdFV0_a",
        "outputId": "7ea176b0-80d1-4f78-f64d-d99d1fddcb7f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20099"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create our model args dict.\n",
        "\n",
        "Use the following as a guide: [Here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L109)"
      ],
      "metadata": {
        "id": "V7bcNelYARmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = dict(\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    block_size=block_size,\n",
        "    bias=bias,\n",
        "    vocab_size=None,\n",
        "    dropout=dropout\n",
        ")"
      ],
      "metadata": {
        "id": "JfIWEbanV7ZS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate our model with the provided `model_args`.\n",
        "\n",
        "These are derived from the hyper-parameters we set above."
      ],
      "metadata": {
        "id": "2WWcbkiCAUI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xly4iA0V-vF",
        "outputId": "8c9bebde-dd4d-4975-e925-9314c04072f1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go! If you used the default values - you should have a model with 29.55M parameters!\n",
        "\n",
        "Let's set our block_size to the correct size as determined in our configuration steps."
      ],
      "metadata": {
        "id": "BpViOsxLAl6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size"
      ],
      "metadata": {
        "id": "TrEawNxdWRhm"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can look at our model in all its glory!"
      ],
      "metadata": {
        "id": "eRgguPLKAuZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaE3KSTnAtJs",
        "outputId": "3720f6dc-ddc3-4b48-e497-a0751fe96a78"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(20099, 516)\n",
              "    (wpe): Embedding(512, 516)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=516, out_features=1548, bias=False)\n",
              "          (c_proj): Linear(in_features=516, out_features=516, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=516, out_features=2064, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=2064, out_features=516, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=516, out_features=20099, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."
      ],
      "metadata": {
        "id": "LzoEY6gcBOSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "BNUThRt4WT5H"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."
      ],
      "metadata": {
        "id": "6Zs5Hcf9BBUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay,\n",
        "    learning_rate,\n",
        "    (beta1, beta2),\n",
        "    device_type\n",
        ")\n",
        "\n",
        "checkpoint = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesGeUnoWViL",
        "outputId": "985d6ebf-4584-45e4-a1c2-bd73bdd0094c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 29,805,708 parameters\n",
            "num non-decayed parameter tensors: 13, with 6,708 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compile our model!\n",
        "\n",
        "If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n",
        "\n",
        "Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
      ],
      "metadata": {
        "id": "ZF5YWJoKB4og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FNU0T0WXdI",
        "outputId": "d15e550d-3879-4be4-fa26-5335459325a9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n",
        "\n",
        "You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."
      ],
      "metadata": {
        "id": "p6lRcVsZCXRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "lUB5zVLVWbhM"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our LR Scheduler\n",
        "\n",
        "Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n",
        "\n",
        "We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n",
        "\n",
        "![img](https://i.imgur.com/KoFEl0b.png)\n",
        "\n",
        "There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"
      ],
      "metadata": {
        "id": "fLsOpaACDDkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "7-mNpWBSWdHh"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set some specific values in our env to allow training in Colab."
      ],
      "metadata": {
        "id": "cqFePCZmE1Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nDL6s4YT6E",
        "outputId": "9171a2fc-b165-4d14-bea7-2206d8e43654"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Loop\n",
        "\n",
        "Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"
      ],
      "metadata": {
        "id": "Nhqmxeo0Eg0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch('train')\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model\n",
        "running_mfu = -1.0 # model flops utilization\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHbyEapRWmpc",
        "outputId": "e0b828a1-7735-4d99-fb74-b8f4bcc1be12"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 10.0518, val loss 10.0589\n",
            "iter 0: loss 10.0317, time 105495.56ms, mfu -100.00%\n",
            "iter 10: loss 8.3868, time 189.10ms, mfu 2.73%\n",
            "iter 20: loss 7.4539, time 192.06ms, mfu 2.72%\n",
            "iter 30: loss 6.3641, time 194.26ms, mfu 2.71%\n",
            "iter 40: loss 5.8285, time 191.77ms, mfu 2.71%\n",
            "iter 50: loss 5.6502, time 192.09ms, mfu 2.71%\n",
            "iter 60: loss 5.3663, time 192.83ms, mfu 2.71%\n",
            "iter 70: loss 5.1313, time 195.25ms, mfu 2.70%\n",
            "iter 80: loss 4.9978, time 193.77ms, mfu 2.70%\n",
            "iter 90: loss 4.8205, time 196.45ms, mfu 2.69%\n",
            "iter 100: loss 4.8238, time 195.12ms, mfu 2.68%\n",
            "iter 110: loss 4.6481, time 196.64ms, mfu 2.68%\n",
            "iter 120: loss 4.5112, time 195.43ms, mfu 2.67%\n",
            "iter 130: loss 4.6038, time 198.89ms, mfu 2.67%\n",
            "iter 140: loss 4.3347, time 198.69ms, mfu 2.66%\n",
            "iter 150: loss 4.4124, time 198.63ms, mfu 2.65%\n",
            "iter 160: loss 4.2674, time 196.46ms, mfu 2.65%\n",
            "iter 170: loss 4.3222, time 196.32ms, mfu 2.65%\n",
            "iter 180: loss 4.3243, time 203.03ms, mfu 2.64%\n",
            "iter 190: loss 4.2032, time 216.88ms, mfu 2.61%\n",
            "iter 200: loss 4.1625, time 199.19ms, mfu 2.61%\n",
            "iter 210: loss 4.0664, time 200.33ms, mfu 2.60%\n",
            "iter 220: loss 4.1680, time 196.37ms, mfu 2.61%\n",
            "iter 230: loss 3.9854, time 204.57ms, mfu 2.60%\n",
            "iter 240: loss 4.1525, time 204.66ms, mfu 2.59%\n",
            "step 250: train loss 4.0030, val loss 4.9450\n",
            "saving checkpoint to out\n",
            "iter 250: loss 4.0873, time 23185.21ms, mfu 2.33%\n",
            "iter 260: loss 3.8326, time 205.03ms, mfu 2.35%\n",
            "iter 270: loss 4.0682, time 209.28ms, mfu 2.36%\n",
            "iter 280: loss 3.8998, time 209.20ms, mfu 2.37%\n",
            "iter 290: loss 3.9569, time 205.56ms, mfu 2.39%\n",
            "iter 300: loss 3.8961, time 205.06ms, mfu 2.40%\n",
            "iter 310: loss 3.8047, time 200.78ms, mfu 2.42%\n",
            "iter 320: loss 3.7985, time 207.03ms, mfu 2.42%\n",
            "iter 330: loss 3.9300, time 201.71ms, mfu 2.44%\n",
            "iter 340: loss 3.7265, time 202.40ms, mfu 2.45%\n",
            "iter 350: loss 3.7361, time 202.51ms, mfu 2.46%\n",
            "iter 360: loss 3.7155, time 204.38ms, mfu 2.46%\n",
            "iter 370: loss 3.7439, time 203.40ms, mfu 2.47%\n",
            "iter 380: loss 3.6548, time 201.88ms, mfu 2.48%\n",
            "iter 390: loss 3.7211, time 198.63ms, mfu 2.49%\n",
            "iter 400: loss 3.5859, time 205.89ms, mfu 2.49%\n",
            "iter 410: loss 3.7380, time 202.77ms, mfu 2.50%\n",
            "iter 420: loss 3.5828, time 202.36ms, mfu 2.50%\n",
            "iter 430: loss 3.4603, time 201.41ms, mfu 2.51%\n",
            "iter 440: loss 3.6115, time 201.31ms, mfu 2.51%\n",
            "iter 450: loss 3.5050, time 201.44ms, mfu 2.52%\n",
            "iter 460: loss 3.6321, time 200.28ms, mfu 2.52%\n",
            "iter 470: loss 3.4379, time 203.38ms, mfu 2.52%\n",
            "iter 480: loss 3.2982, time 204.90ms, mfu 2.52%\n",
            "iter 490: loss 3.3915, time 202.82ms, mfu 2.53%\n",
            "step 500: train loss 3.3441, val loss 5.1525\n",
            "saving checkpoint to out\n",
            "iter 500: loss 3.4345, time 22570.60ms, mfu 2.27%\n",
            "iter 510: loss 3.3910, time 202.99ms, mfu 2.30%\n",
            "iter 520: loss 3.3087, time 201.99ms, mfu 2.33%\n",
            "iter 530: loss 3.4819, time 202.91ms, mfu 2.35%\n",
            "iter 540: loss 3.3166, time 203.05ms, mfu 2.37%\n",
            "iter 550: loss 3.2844, time 203.17ms, mfu 2.38%\n",
            "iter 560: loss 3.2442, time 202.22ms, mfu 2.40%\n",
            "iter 570: loss 3.2739, time 200.84ms, mfu 2.42%\n",
            "iter 580: loss 3.2722, time 203.86ms, mfu 2.43%\n",
            "iter 590: loss 3.3419, time 204.35ms, mfu 2.44%\n",
            "iter 600: loss 2.9809, time 204.64ms, mfu 2.45%\n",
            "iter 610: loss 3.1170, time 203.77ms, mfu 2.45%\n",
            "iter 620: loss 3.2126, time 204.84ms, mfu 2.46%\n",
            "iter 630: loss 3.2212, time 202.26ms, mfu 2.47%\n",
            "iter 640: loss 3.1976, time 200.61ms, mfu 2.48%\n",
            "iter 650: loss 3.0512, time 202.42ms, mfu 2.49%\n",
            "iter 660: loss 3.1119, time 205.25ms, mfu 2.49%\n",
            "iter 670: loss 3.0157, time 202.62ms, mfu 2.49%\n",
            "iter 680: loss 2.9534, time 202.39ms, mfu 2.50%\n",
            "iter 690: loss 2.9858, time 203.06ms, mfu 2.50%\n",
            "iter 700: loss 2.8999, time 202.57ms, mfu 2.51%\n",
            "iter 710: loss 3.0831, time 201.32ms, mfu 2.51%\n",
            "iter 720: loss 2.9712, time 202.41ms, mfu 2.52%\n",
            "iter 730: loss 2.8195, time 203.38ms, mfu 2.52%\n",
            "iter 740: loss 2.8081, time 201.29ms, mfu 2.52%\n",
            "step 750: train loss 2.6689, val loss 5.4618\n",
            "saving checkpoint to out\n",
            "iter 750: loss 2.7944, time 22392.14ms, mfu 2.27%\n",
            "iter 760: loss 2.8212, time 200.60ms, mfu 2.30%\n",
            "iter 770: loss 2.7086, time 201.70ms, mfu 2.33%\n",
            "iter 780: loss 2.8055, time 203.88ms, mfu 2.35%\n",
            "iter 790: loss 2.5958, time 202.77ms, mfu 2.37%\n",
            "iter 800: loss 2.7586, time 201.93ms, mfu 2.39%\n",
            "iter 810: loss 2.5900, time 204.53ms, mfu 2.40%\n",
            "iter 820: loss 2.7625, time 204.71ms, mfu 2.41%\n",
            "iter 830: loss 2.7037, time 204.19ms, mfu 2.42%\n",
            "iter 840: loss 2.6269, time 203.19ms, mfu 2.43%\n",
            "iter 850: loss 2.5920, time 202.95ms, mfu 2.44%\n",
            "iter 860: loss 2.6939, time 203.66ms, mfu 2.45%\n",
            "iter 870: loss 2.6284, time 202.50ms, mfu 2.46%\n",
            "iter 880: loss 2.6288, time 203.53ms, mfu 2.47%\n",
            "iter 890: loss 2.4528, time 201.80ms, mfu 2.48%\n",
            "iter 900: loss 2.4461, time 202.32ms, mfu 2.48%\n",
            "iter 910: loss 2.4148, time 201.90ms, mfu 2.49%\n",
            "iter 920: loss 2.5051, time 203.00ms, mfu 2.50%\n",
            "iter 930: loss 2.3313, time 203.38ms, mfu 2.50%\n",
            "iter 940: loss 2.5602, time 203.60ms, mfu 2.50%\n",
            "iter 950: loss 2.3136, time 202.02ms, mfu 2.51%\n",
            "iter 960: loss 2.4403, time 204.12ms, mfu 2.51%\n",
            "iter 970: loss 2.2842, time 203.19ms, mfu 2.51%\n",
            "iter 980: loss 2.3343, time 202.53ms, mfu 2.52%\n",
            "iter 990: loss 2.1959, time 203.06ms, mfu 2.52%\n",
            "step 1000: train loss 1.9467, val loss 5.8760\n",
            "saving checkpoint to out\n",
            "iter 1000: loss 2.3897, time 27233.83ms, mfu 2.27%\n",
            "iter 1010: loss 2.2184, time 201.28ms, mfu 2.30%\n",
            "iter 1020: loss 1.9611, time 203.11ms, mfu 2.32%\n",
            "iter 1030: loss 2.1660, time 203.95ms, mfu 2.34%\n",
            "iter 1040: loss 2.2957, time 201.32ms, mfu 2.36%\n",
            "iter 1050: loss 2.2562, time 204.33ms, mfu 2.38%\n",
            "iter 1060: loss 2.1349, time 201.75ms, mfu 2.40%\n",
            "iter 1070: loss 2.0455, time 200.47ms, mfu 2.41%\n",
            "iter 1080: loss 1.8909, time 205.72ms, mfu 2.42%\n",
            "iter 1090: loss 1.9904, time 202.58ms, mfu 2.44%\n",
            "iter 1100: loss 2.0867, time 205.08ms, mfu 2.44%\n",
            "iter 1110: loss 1.9880, time 205.50ms, mfu 2.45%\n",
            "iter 1120: loss 1.8991, time 206.33ms, mfu 2.45%\n",
            "iter 1130: loss 1.9164, time 206.30ms, mfu 2.46%\n",
            "iter 1140: loss 1.9843, time 204.24ms, mfu 2.47%\n",
            "iter 1150: loss 1.7941, time 208.80ms, mfu 2.47%\n",
            "iter 1160: loss 2.0047, time 206.71ms, mfu 2.47%\n",
            "iter 1170: loss 1.9383, time 203.91ms, mfu 2.47%\n",
            "iter 1180: loss 1.7470, time 201.34ms, mfu 2.48%\n",
            "iter 1190: loss 1.9093, time 201.88ms, mfu 2.49%\n",
            "iter 1200: loss 1.8310, time 202.24ms, mfu 2.50%\n",
            "iter 1210: loss 1.7517, time 204.90ms, mfu 2.50%\n",
            "iter 1220: loss 1.8269, time 203.14ms, mfu 2.50%\n",
            "iter 1230: loss 1.6904, time 202.63ms, mfu 2.51%\n",
            "iter 1240: loss 1.6429, time 198.73ms, mfu 2.51%\n",
            "step 1250: train loss 1.3143, val loss 6.3232\n",
            "saving checkpoint to out\n",
            "iter 1250: loss 1.7332, time 22117.63ms, mfu 2.27%\n",
            "iter 1260: loss 1.5871, time 199.66ms, mfu 2.30%\n",
            "iter 1270: loss 1.5459, time 203.56ms, mfu 2.32%\n",
            "iter 1280: loss 1.6096, time 203.89ms, mfu 2.34%\n",
            "iter 1290: loss 1.6246, time 204.08ms, mfu 2.36%\n",
            "iter 1300: loss 1.6198, time 203.89ms, mfu 2.38%\n",
            "iter 1310: loss 1.5620, time 201.65ms, mfu 2.39%\n",
            "iter 1320: loss 1.5805, time 203.09ms, mfu 2.41%\n",
            "iter 1330: loss 1.6139, time 205.81ms, mfu 2.42%\n",
            "iter 1340: loss 1.5088, time 202.54ms, mfu 2.43%\n",
            "iter 1350: loss 1.4719, time 202.76ms, mfu 2.44%\n",
            "iter 1360: loss 1.5285, time 202.96ms, mfu 2.45%\n",
            "iter 1370: loss 1.4759, time 204.24ms, mfu 2.46%\n",
            "iter 1380: loss 1.4519, time 203.54ms, mfu 2.47%\n",
            "iter 1390: loss 1.3694, time 202.56ms, mfu 2.47%\n",
            "iter 1400: loss 1.4642, time 203.16ms, mfu 2.48%\n",
            "iter 1410: loss 1.4551, time 202.95ms, mfu 2.49%\n",
            "iter 1420: loss 1.4028, time 200.85ms, mfu 2.49%\n",
            "iter 1430: loss 1.5317, time 204.14ms, mfu 2.50%\n",
            "iter 1440: loss 1.4398, time 201.35ms, mfu 2.50%\n",
            "iter 1450: loss 1.5032, time 202.30ms, mfu 2.51%\n",
            "iter 1460: loss 1.3557, time 202.77ms, mfu 2.51%\n",
            "iter 1470: loss 1.2885, time 202.59ms, mfu 2.51%\n",
            "iter 1480: loss 1.4224, time 202.23ms, mfu 2.52%\n",
            "iter 1490: loss 1.3026, time 203.29ms, mfu 2.52%\n",
            "step 1500: train loss 0.8936, val loss 6.7993\n",
            "saving checkpoint to out\n",
            "iter 1500: loss 1.3110, time 27094.51ms, mfu 2.27%\n",
            "iter 1510: loss 1.3179, time 201.57ms, mfu 2.30%\n",
            "iter 1520: loss 1.1774, time 199.00ms, mfu 2.33%\n",
            "iter 1530: loss 1.2813, time 197.45ms, mfu 2.36%\n",
            "iter 1540: loss 1.3584, time 201.04ms, mfu 2.38%\n",
            "iter 1550: loss 1.4113, time 200.02ms, mfu 2.40%\n",
            "iter 1560: loss 1.2902, time 202.94ms, mfu 2.41%\n",
            "iter 1570: loss 1.3254, time 201.94ms, mfu 2.43%\n",
            "iter 1580: loss 1.2338, time 204.11ms, mfu 2.44%\n",
            "iter 1590: loss 1.2096, time 203.28ms, mfu 2.45%\n",
            "iter 1600: loss 1.2764, time 202.26ms, mfu 2.46%\n",
            "iter 1610: loss 1.1611, time 202.56ms, mfu 2.46%\n",
            "iter 1620: loss 1.1148, time 205.99ms, mfu 2.47%\n",
            "iter 1630: loss 1.3025, time 205.19ms, mfu 2.47%\n",
            "iter 1640: loss 1.2102, time 209.03ms, mfu 2.47%\n",
            "iter 1650: loss 1.0910, time 206.81ms, mfu 2.47%\n",
            "iter 1660: loss 1.2108, time 207.75ms, mfu 2.47%\n",
            "iter 1670: loss 1.1831, time 205.40ms, mfu 2.48%\n",
            "iter 1680: loss 1.0472, time 202.53ms, mfu 2.48%\n",
            "iter 1690: loss 1.1049, time 205.32ms, mfu 2.49%\n",
            "iter 1700: loss 1.0551, time 203.34ms, mfu 2.49%\n",
            "iter 1710: loss 1.1676, time 202.99ms, mfu 2.50%\n",
            "iter 1720: loss 1.0712, time 202.01ms, mfu 2.50%\n",
            "iter 1730: loss 1.0317, time 202.88ms, mfu 2.51%\n",
            "iter 1740: loss 1.0543, time 205.01ms, mfu 2.51%\n",
            "step 1750: train loss 0.6665, val loss 7.2550\n",
            "saving checkpoint to out\n",
            "iter 1750: loss 1.0402, time 26787.59ms, mfu 2.26%\n",
            "iter 1760: loss 1.0481, time 200.23ms, mfu 2.29%\n",
            "iter 1770: loss 0.9542, time 198.10ms, mfu 2.32%\n",
            "iter 1780: loss 1.0669, time 203.89ms, mfu 2.34%\n",
            "iter 1790: loss 1.0920, time 202.42ms, mfu 2.36%\n",
            "iter 1800: loss 1.0660, time 197.77ms, mfu 2.39%\n",
            "iter 1810: loss 0.9683, time 202.73ms, mfu 2.40%\n",
            "iter 1820: loss 0.9657, time 201.83ms, mfu 2.42%\n",
            "iter 1830: loss 1.1535, time 203.18ms, mfu 2.43%\n",
            "iter 1840: loss 1.0586, time 203.86ms, mfu 2.44%\n",
            "iter 1850: loss 1.0236, time 203.16ms, mfu 2.45%\n",
            "iter 1860: loss 0.9788, time 205.95ms, mfu 2.45%\n",
            "iter 1870: loss 1.0370, time 205.16ms, mfu 2.46%\n",
            "iter 1880: loss 0.9774, time 209.15ms, mfu 2.46%\n",
            "iter 1890: loss 0.9293, time 206.73ms, mfu 2.46%\n",
            "iter 1900: loss 0.9332, time 210.13ms, mfu 2.46%\n",
            "iter 1910: loss 0.9480, time 204.58ms, mfu 2.47%\n",
            "iter 1920: loss 0.9267, time 202.51ms, mfu 2.48%\n",
            "iter 1930: loss 0.9086, time 206.40ms, mfu 2.48%\n",
            "iter 1940: loss 0.8626, time 200.83ms, mfu 2.49%\n",
            "iter 1950: loss 0.9651, time 204.48ms, mfu 2.49%\n",
            "iter 1960: loss 0.9203, time 203.58ms, mfu 2.49%\n",
            "iter 1970: loss 0.8759, time 202.14ms, mfu 2.50%\n",
            "iter 1980: loss 0.9312, time 203.54ms, mfu 2.50%\n",
            "iter 1990: loss 0.8622, time 203.39ms, mfu 2.51%\n",
            "step 2000: train loss 0.4918, val loss 7.4907\n",
            "saving checkpoint to out\n",
            "iter 2000: loss 0.9281, time 27061.87ms, mfu 2.26%\n",
            "iter 2010: loss 0.8371, time 199.89ms, mfu 2.29%\n",
            "iter 2020: loss 0.8099, time 199.96ms, mfu 2.32%\n",
            "iter 2030: loss 0.8134, time 201.76ms, mfu 2.34%\n",
            "iter 2040: loss 0.8224, time 201.42ms, mfu 2.36%\n",
            "iter 2050: loss 0.8486, time 200.76ms, mfu 2.38%\n",
            "iter 2060: loss 0.8454, time 201.01ms, mfu 2.40%\n",
            "iter 2070: loss 0.8256, time 203.24ms, mfu 2.42%\n",
            "iter 2080: loss 0.7657, time 202.53ms, mfu 2.43%\n",
            "iter 2090: loss 0.7950, time 200.41ms, mfu 2.44%\n",
            "iter 2100: loss 0.8236, time 207.74ms, mfu 2.45%\n",
            "iter 2110: loss 0.8233, time 207.78ms, mfu 2.45%\n",
            "iter 2120: loss 0.7868, time 206.15ms, mfu 2.46%\n",
            "iter 2130: loss 0.8132, time 208.78ms, mfu 2.46%\n",
            "iter 2140: loss 0.7540, time 202.33ms, mfu 2.47%\n",
            "iter 2150: loss 0.7956, time 208.69ms, mfu 2.47%\n",
            "iter 2160: loss 0.7786, time 207.75ms, mfu 2.47%\n",
            "iter 2170: loss 0.7385, time 204.60ms, mfu 2.47%\n",
            "iter 2180: loss 0.8116, time 203.10ms, mfu 2.48%\n",
            "iter 2190: loss 0.7504, time 205.93ms, mfu 2.48%\n",
            "iter 2200: loss 0.7649, time 203.26ms, mfu 2.49%\n",
            "iter 2210: loss 0.7091, time 202.69ms, mfu 2.49%\n",
            "iter 2220: loss 0.7536, time 202.61ms, mfu 2.50%\n",
            "iter 2230: loss 0.6967, time 201.23ms, mfu 2.50%\n",
            "iter 2240: loss 0.7931, time 203.13ms, mfu 2.51%\n",
            "step 2250: train loss 0.3756, val loss 7.8750\n",
            "saving checkpoint to out\n",
            "iter 2250: loss 0.7526, time 27117.99ms, mfu 2.26%\n",
            "iter 2260: loss 0.7382, time 197.98ms, mfu 2.29%\n",
            "iter 2270: loss 0.7479, time 197.70ms, mfu 2.32%\n",
            "iter 2280: loss 0.7616, time 200.77ms, mfu 2.35%\n",
            "iter 2290: loss 0.6508, time 199.46ms, mfu 2.37%\n",
            "iter 2300: loss 0.7328, time 199.81ms, mfu 2.39%\n",
            "iter 2310: loss 0.6724, time 201.05ms, mfu 2.41%\n",
            "iter 2320: loss 0.6892, time 205.41ms, mfu 2.42%\n",
            "iter 2330: loss 0.7127, time 203.21ms, mfu 2.43%\n",
            "iter 2340: loss 0.6896, time 205.08ms, mfu 2.44%\n",
            "iter 2350: loss 0.6611, time 203.52ms, mfu 2.45%\n",
            "iter 2360: loss 0.6593, time 207.98ms, mfu 2.45%\n",
            "iter 2370: loss 0.6843, time 207.13ms, mfu 2.46%\n",
            "iter 2380: loss 0.6369, time 210.51ms, mfu 2.46%\n",
            "iter 2390: loss 0.6771, time 204.50ms, mfu 2.46%\n",
            "iter 2400: loss 0.6526, time 206.58ms, mfu 2.46%\n",
            "iter 2410: loss 0.6221, time 206.03ms, mfu 2.47%\n",
            "iter 2420: loss 0.5685, time 204.69ms, mfu 2.47%\n",
            "iter 2430: loss 0.6586, time 202.17ms, mfu 2.48%\n",
            "iter 2440: loss 0.5995, time 203.09ms, mfu 2.49%\n",
            "iter 2450: loss 0.5894, time 206.38ms, mfu 2.49%\n",
            "iter 2460: loss 0.6185, time 203.84ms, mfu 2.49%\n",
            "iter 2470: loss 0.6660, time 202.03ms, mfu 2.50%\n",
            "iter 2480: loss 0.5761, time 205.23ms, mfu 2.50%\n",
            "iter 2490: loss 0.6664, time 203.29ms, mfu 2.50%\n",
            "step 2500: train loss 0.2861, val loss 8.1198\n",
            "saving checkpoint to out\n",
            "iter 2500: loss 0.6333, time 27006.50ms, mfu 2.25%\n",
            "iter 2510: loss 0.5654, time 198.94ms, mfu 2.29%\n",
            "iter 2520: loss 0.5781, time 198.65ms, mfu 2.32%\n",
            "iter 2530: loss 0.6399, time 197.83ms, mfu 2.35%\n",
            "iter 2540: loss 0.5901, time 201.67ms, mfu 2.37%\n",
            "iter 2550: loss 0.5662, time 201.24ms, mfu 2.39%\n",
            "iter 2560: loss 0.5558, time 206.71ms, mfu 2.40%\n",
            "iter 2570: loss 0.5986, time 199.24ms, mfu 2.42%\n",
            "iter 2580: loss 0.5564, time 204.85ms, mfu 2.43%\n",
            "iter 2590: loss 0.5444, time 202.14ms, mfu 2.44%\n",
            "iter 2600: loss 0.5587, time 204.04ms, mfu 2.45%\n",
            "iter 2610: loss 0.5475, time 205.12ms, mfu 2.45%\n",
            "iter 2620: loss 0.5401, time 207.12ms, mfu 2.46%\n",
            "iter 2630: loss 0.5659, time 207.04ms, mfu 2.46%\n",
            "iter 2640: loss 0.5181, time 212.82ms, mfu 2.46%\n",
            "iter 2650: loss 0.5701, time 206.56ms, mfu 2.46%\n",
            "iter 2660: loss 0.5246, time 205.98ms, mfu 2.47%\n",
            "iter 2670: loss 0.5418, time 206.05ms, mfu 2.47%\n",
            "iter 2680: loss 0.5338, time 205.51ms, mfu 2.47%\n",
            "iter 2690: loss 0.5378, time 207.10ms, mfu 2.47%\n",
            "iter 2700: loss 0.5178, time 205.00ms, mfu 2.48%\n",
            "iter 2710: loss 0.5324, time 205.84ms, mfu 2.48%\n",
            "iter 2720: loss 0.5287, time 203.17ms, mfu 2.49%\n",
            "iter 2730: loss 0.4950, time 204.63ms, mfu 2.49%\n",
            "iter 2740: loss 0.4775, time 201.91ms, mfu 2.50%\n",
            "step 2750: train loss 0.2236, val loss 8.3837\n",
            "saving checkpoint to out\n",
            "iter 2750: loss 0.4594, time 22345.53ms, mfu 2.25%\n",
            "iter 2760: loss 0.4602, time 201.74ms, mfu 2.28%\n",
            "iter 2770: loss 0.4832, time 201.96ms, mfu 2.31%\n",
            "iter 2780: loss 0.5078, time 200.34ms, mfu 2.33%\n",
            "iter 2790: loss 0.5106, time 200.49ms, mfu 2.36%\n",
            "iter 2800: loss 0.4810, time 199.22ms, mfu 2.38%\n",
            "iter 2810: loss 0.4972, time 202.35ms, mfu 2.40%\n",
            "iter 2820: loss 0.4773, time 204.72ms, mfu 2.41%\n",
            "iter 2830: loss 0.4659, time 204.00ms, mfu 2.42%\n",
            "iter 2840: loss 0.4552, time 200.24ms, mfu 2.44%\n",
            "iter 2850: loss 0.5019, time 204.26ms, mfu 2.44%\n",
            "iter 2860: loss 0.4615, time 201.80ms, mfu 2.46%\n",
            "iter 2870: loss 0.4624, time 201.44ms, mfu 2.47%\n",
            "iter 2880: loss 0.4458, time 199.79ms, mfu 2.48%\n",
            "iter 2890: loss 0.4630, time 200.95ms, mfu 2.49%\n",
            "iter 2900: loss 0.4636, time 208.12ms, mfu 2.49%\n",
            "iter 2910: loss 0.4169, time 202.03ms, mfu 2.49%\n",
            "iter 2920: loss 0.4679, time 198.58ms, mfu 2.50%\n",
            "iter 2930: loss 0.4644, time 206.39ms, mfu 2.50%\n",
            "iter 2940: loss 0.4238, time 204.15ms, mfu 2.50%\n",
            "iter 2950: loss 0.4119, time 202.68ms, mfu 2.51%\n",
            "iter 2960: loss 0.4089, time 204.17ms, mfu 2.51%\n",
            "iter 2970: loss 0.4229, time 204.28ms, mfu 2.51%\n",
            "iter 2980: loss 0.4051, time 202.89ms, mfu 2.51%\n",
            "iter 2990: loss 0.4357, time 202.15ms, mfu 2.52%\n",
            "step 3000: train loss 0.1767, val loss 8.5165\n",
            "saving checkpoint to out\n",
            "iter 3000: loss 0.4171, time 27251.75ms, mfu 2.27%\n",
            "iter 3010: loss 0.4139, time 200.26ms, mfu 2.30%\n",
            "iter 3020: loss 0.4329, time 196.96ms, mfu 2.33%\n",
            "iter 3030: loss 0.4257, time 201.09ms, mfu 2.35%\n",
            "iter 3040: loss 0.3907, time 204.09ms, mfu 2.37%\n",
            "iter 3050: loss 0.3744, time 204.65ms, mfu 2.39%\n",
            "iter 3060: loss 0.3877, time 204.57ms, mfu 2.40%\n",
            "iter 3070: loss 0.3918, time 202.00ms, mfu 2.41%\n",
            "iter 3080: loss 0.4121, time 202.92ms, mfu 2.43%\n",
            "iter 3090: loss 0.3653, time 202.66ms, mfu 2.44%\n",
            "iter 3100: loss 0.4180, time 201.28ms, mfu 2.45%\n",
            "iter 3110: loss 0.3799, time 205.86ms, mfu 2.46%\n",
            "iter 3120: loss 0.4417, time 207.58ms, mfu 2.46%\n",
            "iter 3130: loss 0.3859, time 205.82ms, mfu 2.46%\n",
            "iter 3140: loss 0.4134, time 201.45ms, mfu 2.47%\n",
            "iter 3150: loss 0.3918, time 207.89ms, mfu 2.47%\n",
            "iter 3160: loss 0.3894, time 204.73ms, mfu 2.48%\n",
            "iter 3170: loss 0.4098, time 206.36ms, mfu 2.48%\n",
            "iter 3180: loss 0.3880, time 202.22ms, mfu 2.49%\n",
            "iter 3190: loss 0.3388, time 200.14ms, mfu 2.50%\n",
            "iter 3200: loss 0.3311, time 201.20ms, mfu 2.50%\n",
            "iter 3210: loss 0.3915, time 203.55ms, mfu 2.51%\n",
            "iter 3220: loss 0.3581, time 203.65ms, mfu 2.51%\n",
            "iter 3230: loss 0.3616, time 203.22ms, mfu 2.51%\n",
            "iter 3240: loss 0.3724, time 204.07ms, mfu 2.51%\n",
            "step 3250: train loss 0.1447, val loss 8.6125\n",
            "saving checkpoint to out\n",
            "iter 3250: loss 0.3633, time 26991.09ms, mfu 2.26%\n",
            "iter 3260: loss 0.3765, time 198.60ms, mfu 2.30%\n",
            "iter 3270: loss 0.3580, time 199.67ms, mfu 2.32%\n",
            "iter 3280: loss 0.3425, time 200.16ms, mfu 2.35%\n",
            "iter 3290: loss 0.3582, time 198.69ms, mfu 2.37%\n",
            "iter 3300: loss 0.3583, time 216.83ms, mfu 2.37%\n",
            "iter 3310: loss 0.3594, time 203.01ms, mfu 2.39%\n",
            "iter 3320: loss 0.3499, time 202.17ms, mfu 2.41%\n",
            "iter 3330: loss 0.3734, time 201.43ms, mfu 2.42%\n",
            "iter 3340: loss 0.3356, time 200.79ms, mfu 2.44%\n",
            "iter 3350: loss 0.3572, time 205.14ms, mfu 2.44%\n",
            "iter 3360: loss 0.3437, time 203.07ms, mfu 2.45%\n",
            "iter 3370: loss 0.3640, time 209.45ms, mfu 2.45%\n",
            "iter 3380: loss 0.3149, time 207.70ms, mfu 2.46%\n",
            "iter 3390: loss 0.3177, time 206.27ms, mfu 2.46%\n",
            "iter 3400: loss 0.3282, time 208.14ms, mfu 2.46%\n",
            "iter 3410: loss 0.3503, time 204.60ms, mfu 2.47%\n",
            "iter 3420: loss 0.3171, time 204.48ms, mfu 2.47%\n",
            "iter 3430: loss 0.3279, time 203.02ms, mfu 2.48%\n",
            "iter 3440: loss 0.3303, time 200.69ms, mfu 2.49%\n",
            "iter 3450: loss 0.3281, time 202.20ms, mfu 2.49%\n",
            "iter 3460: loss 0.3394, time 208.27ms, mfu 2.49%\n",
            "iter 3470: loss 0.3393, time 204.30ms, mfu 2.50%\n",
            "iter 3480: loss 0.3416, time 201.74ms, mfu 2.50%\n",
            "iter 3490: loss 0.3195, time 202.08ms, mfu 2.51%\n",
            "step 3500: train loss 0.1220, val loss 8.7964\n",
            "saving checkpoint to out\n",
            "iter 3500: loss 0.3030, time 22979.40ms, mfu 2.26%\n",
            "iter 3510: loss 0.3237, time 198.62ms, mfu 2.29%\n",
            "iter 3520: loss 0.3220, time 199.40ms, mfu 2.32%\n",
            "iter 3530: loss 0.3173, time 202.33ms, mfu 2.34%\n",
            "iter 3540: loss 0.3124, time 199.36ms, mfu 2.37%\n",
            "iter 3550: loss 0.3086, time 199.66ms, mfu 2.39%\n",
            "iter 3560: loss 0.2976, time 205.27ms, mfu 2.40%\n",
            "iter 3570: loss 0.2920, time 203.06ms, mfu 2.42%\n",
            "iter 3580: loss 0.2856, time 200.33ms, mfu 2.43%\n",
            "iter 3590: loss 0.3137, time 201.62ms, mfu 2.44%\n",
            "iter 3600: loss 0.3090, time 199.18ms, mfu 2.46%\n",
            "iter 3610: loss 0.3142, time 202.32ms, mfu 2.47%\n",
            "iter 3620: loss 0.2934, time 203.18ms, mfu 2.47%\n",
            "iter 3630: loss 0.3096, time 200.72ms, mfu 2.48%\n",
            "iter 3640: loss 0.2882, time 201.82ms, mfu 2.49%\n",
            "iter 3650: loss 0.2846, time 201.56ms, mfu 2.50%\n",
            "iter 3660: loss 0.2855, time 198.48ms, mfu 2.51%\n",
            "iter 3670: loss 0.2997, time 203.51ms, mfu 2.51%\n",
            "iter 3680: loss 0.2695, time 201.93ms, mfu 2.51%\n",
            "iter 3690: loss 0.2643, time 200.97ms, mfu 2.52%\n",
            "iter 3700: loss 0.2834, time 202.04ms, mfu 2.52%\n",
            "iter 3710: loss 0.2584, time 200.50ms, mfu 2.53%\n",
            "iter 3720: loss 0.2417, time 201.64ms, mfu 2.53%\n",
            "iter 3730: loss 0.2858, time 203.54ms, mfu 2.53%\n",
            "iter 3740: loss 0.2686, time 203.33ms, mfu 2.53%\n",
            "step 3750: train loss 0.1037, val loss 8.9299\n",
            "saving checkpoint to out\n",
            "iter 3750: loss 0.2610, time 22586.04ms, mfu 2.28%\n",
            "iter 3760: loss 0.2840, time 201.11ms, mfu 2.31%\n",
            "iter 3770: loss 0.2634, time 201.46ms, mfu 2.33%\n",
            "iter 3780: loss 0.2868, time 202.06ms, mfu 2.36%\n",
            "iter 3790: loss 0.2588, time 203.85ms, mfu 2.37%\n",
            "iter 3800: loss 0.2677, time 204.44ms, mfu 2.39%\n",
            "iter 3810: loss 0.2512, time 201.25ms, mfu 2.40%\n",
            "iter 3820: loss 0.2755, time 201.09ms, mfu 2.42%\n",
            "iter 3830: loss 0.2929, time 201.46ms, mfu 2.43%\n",
            "iter 3840: loss 0.2702, time 203.20ms, mfu 2.44%\n",
            "iter 3850: loss 0.2518, time 202.61ms, mfu 2.45%\n",
            "iter 3860: loss 0.2536, time 202.85ms, mfu 2.46%\n",
            "iter 3870: loss 0.2562, time 201.58ms, mfu 2.47%\n",
            "iter 3880: loss 0.2743, time 202.68ms, mfu 2.48%\n",
            "iter 3890: loss 0.2472, time 200.54ms, mfu 2.49%\n",
            "iter 3900: loss 0.2681, time 202.65ms, mfu 2.49%\n",
            "iter 3910: loss 0.2489, time 201.30ms, mfu 2.50%\n",
            "iter 3920: loss 0.2625, time 202.82ms, mfu 2.50%\n",
            "iter 3930: loss 0.2674, time 200.20ms, mfu 2.51%\n",
            "iter 3940: loss 0.2629, time 203.29ms, mfu 2.51%\n",
            "iter 3950: loss 0.2503, time 201.97ms, mfu 2.52%\n",
            "iter 3960: loss 0.2468, time 205.02ms, mfu 2.52%\n",
            "iter 3970: loss 0.2468, time 199.91ms, mfu 2.52%\n",
            "iter 3980: loss 0.2593, time 202.10ms, mfu 2.53%\n",
            "iter 3990: loss 0.2340, time 202.21ms, mfu 2.53%\n",
            "step 4000: train loss 0.0940, val loss 8.9850\n",
            "saving checkpoint to out\n",
            "iter 4000: loss 0.2441, time 27225.33ms, mfu 2.28%\n",
            "iter 4010: loss 0.2257, time 196.96ms, mfu 2.31%\n",
            "iter 4020: loss 0.2452, time 196.38ms, mfu 2.34%\n",
            "iter 4030: loss 0.2212, time 200.07ms, mfu 2.37%\n",
            "iter 4040: loss 0.2266, time 199.05ms, mfu 2.39%\n",
            "iter 4050: loss 0.2169, time 199.57ms, mfu 2.41%\n",
            "iter 4060: loss 0.2155, time 201.84ms, mfu 2.42%\n",
            "iter 4070: loss 0.2461, time 202.11ms, mfu 2.44%\n",
            "iter 4080: loss 0.2267, time 203.25ms, mfu 2.45%\n",
            "iter 4090: loss 0.2350, time 203.36ms, mfu 2.45%\n",
            "iter 4100: loss 0.2426, time 208.94ms, mfu 2.46%\n",
            "iter 4110: loss 0.2349, time 207.37ms, mfu 2.46%\n",
            "iter 4120: loss 0.2185, time 207.73ms, mfu 2.46%\n",
            "iter 4130: loss 0.2512, time 207.55ms, mfu 2.46%\n",
            "iter 4140: loss 0.2253, time 202.83ms, mfu 2.47%\n",
            "iter 4150: loss 0.2195, time 201.63ms, mfu 2.48%\n",
            "iter 4160: loss 0.2390, time 206.96ms, mfu 2.48%\n",
            "iter 4170: loss 0.2421, time 203.44ms, mfu 2.49%\n",
            "iter 4180: loss 0.2316, time 199.96ms, mfu 2.50%\n",
            "iter 4190: loss 0.2304, time 200.95ms, mfu 2.50%\n",
            "iter 4200: loss 0.2341, time 202.76ms, mfu 2.51%\n",
            "iter 4210: loss 0.2343, time 204.73ms, mfu 2.51%\n",
            "iter 4220: loss 0.2178, time 203.59ms, mfu 2.51%\n",
            "iter 4230: loss 0.2482, time 201.81ms, mfu 2.51%\n",
            "iter 4240: loss 0.2135, time 202.49ms, mfu 2.52%\n",
            "step 4250: train loss 0.0855, val loss 9.0446\n",
            "saving checkpoint to out\n",
            "iter 4250: loss 0.2351, time 26968.54ms, mfu 2.27%\n",
            "iter 4260: loss 0.2188, time 198.53ms, mfu 2.30%\n",
            "iter 4270: loss 0.2310, time 199.09ms, mfu 2.33%\n",
            "iter 4280: loss 0.2108, time 198.56ms, mfu 2.36%\n",
            "iter 4290: loss 0.2230, time 200.93ms, mfu 2.38%\n",
            "iter 4300: loss 0.2147, time 201.08ms, mfu 2.40%\n",
            "iter 4310: loss 0.2142, time 199.95ms, mfu 2.41%\n",
            "iter 4320: loss 0.2198, time 202.08ms, mfu 2.43%\n",
            "iter 4330: loss 0.2083, time 204.06ms, mfu 2.44%\n",
            "iter 4340: loss 0.2047, time 204.19ms, mfu 2.45%\n",
            "iter 4350: loss 0.2114, time 204.01ms, mfu 2.45%\n",
            "iter 4360: loss 0.2223, time 204.53ms, mfu 2.46%\n",
            "iter 4370: loss 0.1713, time 205.03ms, mfu 2.47%\n",
            "iter 4380: loss 0.2149, time 207.47ms, mfu 2.47%\n",
            "iter 4390: loss 0.2070, time 207.56ms, mfu 2.47%\n",
            "iter 4400: loss 0.2132, time 207.99ms, mfu 2.47%\n",
            "iter 4410: loss 0.2217, time 208.22ms, mfu 2.47%\n",
            "iter 4420: loss 0.2160, time 204.01ms, mfu 2.48%\n",
            "iter 4430: loss 0.2145, time 206.15ms, mfu 2.48%\n",
            "iter 4440: loss 0.2018, time 203.34ms, mfu 2.48%\n",
            "iter 4450: loss 0.1940, time 202.53ms, mfu 2.49%\n",
            "iter 4460: loss 0.2164, time 203.21ms, mfu 2.50%\n",
            "iter 4470: loss 0.1960, time 202.42ms, mfu 2.50%\n",
            "iter 4480: loss 0.2026, time 201.20ms, mfu 2.51%\n",
            "iter 4490: loss 0.1913, time 201.67ms, mfu 2.51%\n",
            "step 4500: train loss 0.0787, val loss 9.1565\n",
            "saving checkpoint to out\n",
            "iter 4500: loss 0.1962, time 26804.05ms, mfu 2.26%\n",
            "iter 4510: loss 0.2142, time 199.15ms, mfu 2.29%\n",
            "iter 4520: loss 0.2097, time 197.72ms, mfu 2.33%\n",
            "iter 4530: loss 0.1801, time 200.94ms, mfu 2.35%\n",
            "iter 4540: loss 0.1960, time 199.35ms, mfu 2.37%\n",
            "iter 4550: loss 0.1993, time 197.64ms, mfu 2.40%\n",
            "iter 4560: loss 0.2008, time 202.74ms, mfu 2.41%\n",
            "iter 4570: loss 0.2084, time 201.54ms, mfu 2.43%\n",
            "iter 4580: loss 0.2009, time 202.67ms, mfu 2.44%\n",
            "iter 4590: loss 0.1980, time 203.96ms, mfu 2.45%\n",
            "iter 4600: loss 0.1874, time 203.73ms, mfu 2.46%\n",
            "iter 4610: loss 0.2163, time 207.22ms, mfu 2.46%\n",
            "iter 4620: loss 0.2140, time 205.69ms, mfu 2.46%\n",
            "iter 4630: loss 0.2224, time 209.51ms, mfu 2.46%\n",
            "iter 4640: loss 0.1734, time 206.55ms, mfu 2.47%\n",
            "iter 4650: loss 0.1785, time 207.38ms, mfu 2.47%\n",
            "iter 4660: loss 0.2075, time 210.45ms, mfu 2.47%\n",
            "iter 4670: loss 0.1770, time 205.48ms, mfu 2.47%\n",
            "iter 4680: loss 0.1942, time 202.77ms, mfu 2.48%\n",
            "iter 4690: loss 0.1982, time 207.19ms, mfu 2.48%\n",
            "iter 4700: loss 0.2111, time 203.40ms, mfu 2.48%\n",
            "iter 4710: loss 0.2055, time 207.30ms, mfu 2.48%\n",
            "iter 4720: loss 0.2061, time 202.84ms, mfu 2.49%\n",
            "iter 4730: loss 0.2023, time 203.25ms, mfu 2.49%\n",
            "iter 4740: loss 0.1901, time 203.23ms, mfu 2.50%\n",
            "step 4750: train loss 0.0750, val loss 9.1550\n",
            "saving checkpoint to out\n",
            "iter 4750: loss 0.1867, time 27127.68ms, mfu 2.25%\n",
            "iter 4760: loss 0.1970, time 197.91ms, mfu 2.29%\n",
            "iter 4770: loss 0.2235, time 199.54ms, mfu 2.32%\n",
            "iter 4780: loss 0.1992, time 197.05ms, mfu 2.35%\n",
            "iter 4790: loss 0.1782, time 201.28ms, mfu 2.37%\n",
            "iter 4800: loss 0.2062, time 199.96ms, mfu 2.39%\n",
            "iter 4810: loss 0.1816, time 197.58ms, mfu 2.41%\n",
            "iter 4820: loss 0.1838, time 201.96ms, mfu 2.42%\n",
            "iter 4830: loss 0.1834, time 214.04ms, mfu 2.42%\n",
            "iter 4840: loss 0.1896, time 203.85ms, mfu 2.43%\n",
            "iter 4850: loss 0.2052, time 205.18ms, mfu 2.44%\n",
            "iter 4860: loss 0.1839, time 207.98ms, mfu 2.45%\n",
            "iter 4870: loss 0.1783, time 205.72ms, mfu 2.45%\n",
            "iter 4880: loss 0.1936, time 208.07ms, mfu 2.45%\n",
            "iter 4900: loss 0.2017, time 202.52ms, mfu 2.47%\n",
            "iter 4910: loss 0.1984, time 208.11ms, mfu 2.47%\n",
            "iter 4920: loss 0.2104, time 205.35ms, mfu 2.47%\n",
            "iter 4930: loss 0.1744, time 201.08ms, mfu 2.48%\n",
            "iter 4940: loss 0.1838, time 202.07ms, mfu 2.49%\n",
            "iter 4950: loss 0.1778, time 203.15ms, mfu 2.49%\n",
            "iter 4960: loss 0.1987, time 202.07ms, mfu 2.50%\n",
            "iter 4970: loss 0.1866, time 204.83ms, mfu 2.50%\n",
            "iter 4980: loss 0.1898, time 201.13ms, mfu 2.51%\n",
            "iter 4990: loss 0.1698, time 205.46ms, mfu 2.51%\n",
            "step 5000: train loss 0.0722, val loss 9.2033\n",
            "saving checkpoint to out\n",
            "iter 5000: loss 0.1807, time 26783.60ms, mfu 2.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Outputs with our New Model\n",
        "\n",
        "Now we can leverage the `sample.py` file to generate outputs from our model!"
      ],
      "metadata": {
        "id": "L2J5JlRxFJOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation Set Up and Model Loading"
      ],
      "metadata": {
        "id": "eo_QP1ITFfX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "-vftqU9LheEK"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRB3j7iiNkl",
        "outputId": "725eefcd-2571-4dc4-bcaa-cfd0d5f809ee"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ],
      "metadata": {
        "id": "N1YAy8DriVZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tokenizer\n",
        "encode = lambda s: enc.encode(s)\n",
        "decode = lambda l: enc.decode(l)"
      ],
      "metadata": {
        "id": "KoB-5ZuLicAT"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation!"
      ],
      "metadata": {
        "id": "mkTQ9wo7FjYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmTcaHCjii5l",
        "outputId": "fe3bd805-7b8f-4817-edd6-de051fe97514"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To the noble lords, in safety,\n",
            "And from whence I have safety lies together\n",
            "End with post to hope of the noble queen\n",
            ".\n",
            "\n",
            "CLARENCE:\n",
            "I'll take the queen.\n",
            "\n",
            "GLOUCESTER:\n",
            "And Clarence, I hold my soul to lie in charge of this night.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Well, princely nephew, brothers both gone to the loving kiss I hold.\n",
            "\n",
            "CLARENCE:\n",
            "Now, I seated as my soul delights,\n",
            "Having my country's peace and brothers' loves.\n",
            "\n",
            "KING EDWARD IV:\n",
            "What will your chief that the Lady Grey?\n",
            "Set am I seated in charge you\n",
            "Reignier, to the king of France\n",
            "Hath pawn'd the Sicils and Jerusalem,\n",
            "And hither have they sent it for her ransom.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Away with her, and waft her hence to France.\n",
            "And now what rests but that we spend the time\n",
            "With stately triumphs, mirthful comic shows,\n",
            "Such as befits the pleasure of the court?\n",
            "Sound drums and trumpets! farewell sour annoy!\n",
            "For here, I hope, begins our lasting joy.\n",
            "\n",
            "ARCHIDAMUS:\n",
            "If you shall chance, Camillo, to visit Bohemia, on\n",
            "the like occasion whereon my services are now on\n",
            "foot, you shall see, as I have said, great\n",
            "difference betwixt our Bohemia and your Sicilia.\n",
            "\n",
            "CAMILLO:\n",
            "I think, this coming summer, the King of Sicilia\n",
            "means to pay Bohemia the visitation which he justly owes him.\n",
            "\n",
            "ARCHIDAMUS:\n",
            "Wherein our entertainment shall shame us we will be\n",
            "justified in our loves; for indeed--\n",
            "\n",
            "CAMILLO:\n",
            "Beseech you,--\n",
            "\n",
            "ARCHIDAMUS:\n",
            "Verily, I speak it in the freedom of my knowledge:\n",
            "we cannot with such magnificence--in so rare--I know\n",
            "not what to say. We will give you sleepy drinks,\n",
            "that your senses, unintelligent of our insufficience,\n",
            "may, though they cannot praise us, though they cannot praise us, as little accuse\n",
            "us.\n",
            "\n",
            "CAMILLO:\n",
            "You pay a great deal too dear for what's given freely.\n",
            "\n",
            "ARCHIDAMUS:\n",
            "Believe me, I speak as my understanding instructs me\n",
            "and as mine honesty puts it to utterance.\n",
            "---------------\n",
            "\n",
            "\n",
            "KING RICHARD III:\n",
            "Say that, madam; and I have been in haste.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "But how long shall her sweet lie last?\n",
            "\n",
            "KING RICHARD III:\n",
            "Say, madam,\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "How canst thou woo her?\n",
            "\n",
            "KING RICHARD III:\n",
            "That would I learn of you,\n",
            "As one that are best acquainted with her humour.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "And wilt thou learn of me?\n",
            "\n",
            "KING RICHARD III:\n",
            "Madam, with all my heart.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Send to her, by the man that slew her brothers,\n",
            "A pair of bleeding-hearts; thereon engrave\n",
            "Edward and York; then haply she will weep:\n",
            "Therefore present to her--as sometime Margaret\n",
            "Did to thy father, steep'd in Rutland's blood,--\n",
            "A handkerchief; which, say to her, did drain\n",
            "The purple sap from her sweet brother's body\n",
            "And bid her dry her weeping eyes therewith.\n",
            "If this inducement force her not to love,\n",
            "Send her a story of thy noble acts;\n",
            "Tell her thou madest away her uncle Clarence,\n",
            "Her uncle Rivers; yea, and, for her sake,\n",
            "Madest quick conveyance with her good aunt Anne.\n",
            "\n",
            "KING RICHARD III:\n",
            "Come, come, you mock me; this is not the way\n",
            "To win our daughter.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "There is no other way\n",
            "Unless thou couldst put on some other shape,\n",
            "And not be Richard that hath done all this.\n",
            "\n",
            "KING RICHARD III:\n",
            "Say that I did all this for love of her.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Nay, then indeed she cannot choose but hate thee,\n",
            "Having bought love with such a bloody spoil.\n",
            "\n",
            "KING RICHARD III:\n",
            "Look, what is done cannot be now amended:\n",
            "Men shall deal unadvisedly sometimes,\n",
            "Which after hours give leisure to repent.\n",
            "If I did take the kingdom from your sons,\n",
            "To make amends, Ill give it to your daughter.\n",
            "If I have kill'd the issue of your womb,\n",
            "To quicken your increase, I will beget\n",
            "Mine issue of your blood upon your blood upon your daughter\n",
            "A grandam's name is little less in love\n",
            "Than is the doting title of\n",
            "---------------\n",
            "\n",
            "ISABELLA:\n",
            "The image of him, but his life, from heaven,\n",
            "As I would give speak not quickly.\n",
            "\n",
            "ANGELO:\n",
            "Look, what I'll not, that; that I can speak\n",
            "And you shall die for that;\n",
            "And rather proved the sliding of your brother\n",
            "A merriment than a vice.\n",
            "\n",
            "ISABELLA:\n",
            "O, pardon me, my lord; it oft falls out,\n",
            "To have what we would have, we speak not what we mean:\n",
            "I something do excuse the thing I hate,\n",
            "For his advantage that I dearly love.\n",
            "\n",
            "ANGELO:\n",
            "We are all frail.\n",
            "\n",
            "ISABELLA:\n",
            "Else let my brother die,\n",
            "If not a feodary, but only he\n",
            "Owe and succeed thy weakness.\n",
            "\n",
            "ANGELO:\n",
            "Nay, women are frail too.\n",
            "\n",
            "ISABELLA:\n",
            "Ay, as the glasses where they view themselves;\n",
            "Which are as easy broke as they make forms.\n",
            "Women! Help Heaven! men their creation mar\n",
            "In profiting by them. Nay, call us ten times frail;\n",
            "For we are soft as our complexions are,\n",
            "And credulous to false prints.\n",
            "\n",
            "ANGELO:\n",
            "I think it well:\n",
            "And from this testimony of your own sex,--\n",
            "Since I suppose we are made to be no stronger\n",
            "Than faults may shake our frames,--let me be bold;\n",
            "I do arrest your words. Be that you are, a woman;\n",
            "That is, a woman; if you be more, you be more, you be more,\n",
            "If you be one, you are well express'd\n",
            "By all external warrants, show it now,\n",
            "By putting on the destined livery.\n",
            "\n",
            "ISABELLA:\n",
            "I have no tongue but one: gentle my lord,\n",
            "Let me entreat you speak the former language.\n",
            "\n",
            "ANGELO:\n",
            "Plainly conceive, I love you.\n",
            "\n",
            "ISABELLA:\n",
            "My brother did love Juliet,\n",
            "And you tell me that he shall die for it.\n",
            "\n",
            "ANGELO:\n",
            "He shall not, Isabel, if you give me love.\n",
            "\n",
            "ISABELLA:\n",
            "I know your virtue hath a licence in't,\n",
            "Which seems a little fouler than it is,\n",
            "To pluck on others.\n",
            "\n",
            "AN\n",
            "---------------\n",
            "\n",
            "\n",
            "KING RICHARD III:\n",
            "I know the stroke of ten.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Then he is most sovereign.\n",
            "\n",
            "KING RICHARD III:\n",
            "Well, be a high tale.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Send to her my loving tale.\n",
            "\n",
            "KING RICHARD III:\n",
            "Say, even the man that's son:\n",
            " by her her her her her--\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "As I, and mighty and mighty souls\n",
            "And be the accent of bleeding-born.\n",
            "\n",
            "KING RICHARD III:\n",
            "Harp not to her my true blood.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "So, indeed--\n",
            "\n",
            "KING RICHARD III:\n",
            "By nothing; for her my garter, and my crown,--\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "I swear--\n",
            "\n",
            "KING RICHARD III:\n",
            "I swear--\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Thyself thyself misusest.\n",
            "\n",
            "KING RICHARD III:\n",
            "Why then, by God--\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "God's wrong is most of all.\n",
            "If thou hadst fear'd to break an oath by Him,\n",
            "The unity the king thy brother made\n",
            "Had not been broken, nor my brother slain:\n",
            "If thou hadst fear'd to break an oath by Him,\n",
            "The imperial metal, circling now thy brow,\n",
            "Had graced the tender temples of my child,\n",
            "And both the princes had been breathing here,\n",
            "Which now, two tender playfellows to dust,\n",
            "Thy broken faith hath made a prey for worms.\n",
            "What canst thou swear by now?\n",
            "\n",
            "KING RICHARD III:\n",
            "The time to come.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "That thou hast wronged in the time o'erpast;\n",
            "For I myself have many tears to wash\n",
            "Hereafter time, for time past wrong'd by time past wrong'd by thee.\n",
            "The children live, whose parents thou hast\n",
            "slaughter'd,\n",
            "Ungovern'd youth, to wail it in their age;\n",
            "The parents live, whose children thou hast butcher'd,\n",
            "Old wither'd plants, to wail it with their age.\n",
            "Swear not by time to come; for that thou hast\n",
            "Misused ere used, by time misused o'erpast.\n",
            "\n",
            "KING RICHARD III:\n",
            "As I intend to prosper and repent,\n",
            "So thrive I in my dangerous attempt\n",
            "Of hostile arms! myself confound!\n",
            "H\n",
            "---------------\n",
            "\n",
            "\n",
            "MAMILLIUS:\n",
            "Ay, my good lord.\n",
            "\n",
            "LEONTES:\n",
            "Why, what?\n",
            "\n",
            "CAMILLO:\n",
            "Ay, my good lord.\n",
            "\n",
            "LEONTES:\n",
            "Go play, Mamillius; thou'rt an honest man.\n",
            "Camillo, this great sir will yet stay longer.\n",
            "\n",
            "CAMILLO:\n",
            "You had much ado to make his anchor hold:\n",
            "When you cast out, it still came home.\n",
            "\n",
            "LEONTES:\n",
            "Didst note it?\n",
            "\n",
            "CAMILLO:\n",
            "He would not stay at your petitions: made\n",
            "His business more material.\n",
            "\n",
            "LEONTES:\n",
            "Didst perceive it?\n",
            "They're here with me already, whispering, rounding\n",
            "'Sicilia is a so-forth:' 'tis far gone,\n",
            "When I shall gust it last. How came't, Camillo,\n",
            "That he did stay?\n",
            "\n",
            "CAMILLO:\n",
            "At the good queen's entreaty.\n",
            "\n",
            "LEONTES:\n",
            "At the queen's be't: 'good' should be pertinent\n",
            "But, so it is, it is, it is not. Was this taken\n",
            "By any understanding pate but thine?\n",
            "For thy conceit is soaking, will draw in\n",
            "More than the common blocks: not noted, is't,\n",
            "But of the finer natures? by some severals\n",
            "Of head-piece extraordinary? lower messes\n",
            "Perchance are to this business purblind? say.\n",
            "\n",
            "CAMILLO:\n",
            "Business, my lord! I think most understand\n",
            "Bohemia stays here longer.\n",
            "\n",
            "LEONTES:\n",
            "Ha!\n",
            "\n",
            "CAMILLO:\n",
            "Stays here longer.\n",
            "\n",
            "LEONTES:\n",
            "Ay, but why?\n",
            "\n",
            "CAMILLO:\n",
            "To satisfy your highness and the entreaties\n",
            "Of our most gracious mistress.\n",
            "\n",
            "LEONTES:\n",
            "Satisfy!\n",
            "The entreaties of your mistress! satisfy!\n",
            "Let that suffice. I have trusted thee, Camillo,\n",
            "With all the nearest things to my heart, as well\n",
            "My chamber-councils, wherein, priest-like, priest-like, thou\n",
            "Hast cleansed my bosom, I from thee departed\n",
            "Thy penitent reform'd: but we have been\n",
            "Deceived in thy integrity, deceived\n",
            "In that which seems so.\n",
            "\n",
            "CAMILLO:\n",
            "Be it forbid, my lord!\n",
            "\n",
            "LE\n",
            "---------------\n",
            "\n",
            "First Murderer:\n",
            "Take him to kill him. Masters all!\n",
            "\n",
            "AUFIDIUS:\n",
            "My lords, hear me speak. Masters all, be quiet;\n",
            "Provoked by him, you cannot--the great danger\n",
            "Which this man's life did owe you, you'll rejoice\n",
            "That he is thus cut off. Please it your honours\n",
            "To call me to your senate, I'll deliver\n",
            "Myself your loyal servant, or endure\n",
            "Your heaviest censure.\n",
            "\n",
            "First Lord:\n",
            "Bear from hence his body;\n",
            "And mourn you for him: let him be regarded\n",
            "As the most noble corse that ever herald\n",
            "Did follow to his urn.\n",
            "Second Lord:\n",
            "His own impatience\n",
            "Takes from Aufidius a great part of blame.\n",
            "Let's make the best of it.\n",
            "\n",
            "AUFIDIUS:\n",
            "My rage is gone;\n",
            "And I am struck with sorrow. Take him up.\n",
            "Help, three o' the chiefest soldiers; I'll be one.\n",
            "Beat thou the drum, that it speak mournfully:\n",
            "Trail your steel pikes. Though in this city he\n",
            "Hath widow'd and unchilded many a one,\n",
            "Which to this hour bewail the injury,\n",
            "Yet he shall have a noble memory. Assist.\n",
            "\n",
            "GLOUCESTER:\n",
            "Now is the winter of our discontent\n",
            "Made glorious summer by this sun of York;\n",
            "And all the clouds that lour'd upon our house\n",
            "In the deep bosom of the ocean buried.\n",
            "Now are our brows bound with victorious wreaths;\n",
            "Our bruised arms hung up for monuments;\n",
            "Our stern alarums changed to merry meetings,\n",
            "Our dreadful marches to delightful measures.\n",
            "Grim-visaged war hath smooth'd his wrinkled front;\n",
            "And now, instead of mounting barded steeds\n",
            "To fright the souls of fearful adversaries,\n",
            "He capers nimbly in a lady's chamber\n",
            "To the lascivious pleasing of a lute.\n",
            "But I, that am not shaped for sportive tricks,\n",
            "Nor made to court an amorous looking-glass;\n",
            "I, that am rudely stamp'd, and want love's majesty\n",
            "To strut before a wanton ambling nymph;\n",
            "I, that am curtail'd of this fair proportion,\n",
            "Cheated of feature by dissembling nature,\n",
            "Deformed, unfinish'd, sent before my time\n",
            "I\n",
            "---------------\n",
            "\n",
            "The silence and the more absolute power,\n",
            "Than the truth of the nobleness; who care\n",
            "I say, your father's blest,\n",
            "Which to the thronging troops that bears; for, one of what\n",
            "Desom I being sued to aught;\n",
            "For one of what was, one, one, one, one, one, one, one,\n",
            "To choke it in the utterance.\n",
            "\n",
            "First Lord:\n",
            "IBeing a more, another, the interpretation of the time\n",
            "And liberty, the interpretation of the time\n",
            "And not the time: but death,\n",
            "Now it bears; and quite athwart\n",
            "We all, in execution\n",
            "Goes all decorum.\n",
            "\n",
            "First Senator:\n",
            "It shall proceed\n",
            "To unloose this tied-up justice when you pleased:\n",
            "And it in you more dreadful would have seem'd\n",
            "Than in Lord Angelo.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I do fear, too dreadful:\n",
            "Sith 'twas my fault to give the people scope,\n",
            "'Twould be my tyranny to strike and gall them\n",
            "For what I bid them do: for we bid this be done,\n",
            "When evil deeds have their permissive pass\n",
            "And not the punishment. Therefore indeed, my father,\n",
            "I have on Angelo imposed the office;\n",
            "Who may, in the ambush of my name, strike home,\n",
            "And yet my nature never in the fight\n",
            "To do in slander. And to behold his sway,\n",
            "I will, as 'twere a brother of your order,\n",
            "Visit both prince and people: therefore, I prithee,\n",
            "Supply me with the habit and instruct me\n",
            "How I may formally in person bear me\n",
            "Like a true friar. More reasons for this action\n",
            "At our more leisure shall I render you;\n",
            "Only, this one: Lord Angelo is precise;\n",
            "Stands at a guard with envy; scarce confesses\n",
            "That his blood flows, or that his appetite\n",
            "Is more to bread than stone: hence shall we see, what our seemers be.\n",
            "\n",
            "ISABELLA:\n",
            "And have you nuns no farther privileges?\n",
            "\n",
            "FRANCISCA:\n",
            "Are not these large enough?\n",
            "\n",
            "ISABELLA:\n",
            "Yes, truly; I speak not as desiring more;\n",
            "But rather wishing a more strict restraint\n",
            "Upon the sisterhood, the votarists of Saint Clare.\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "Thus, with thy good old folks and let them tell thee tales.\n",
            "\n",
            "Hard- folks, look thee tales.\n",
            "\n",
            "QUEEN:\n",
            "Of any grievous tales of me;\n",
            "For of me well, girl.\n",
            "\n",
            "LUCIO:\n",
            "Of the hill, for loving, as I had as lief be true.\n",
            "\n",
            "GREMIO:\n",
            "Of love, here be, myself, as one doth so that art the\n",
            "art piled, for a French velvet. Do I speak\n",
            "feelingly now?\n",
            "\n",
            "LUCIO:\n",
            "I think thou dost; and, indeed, with most painful\n",
            "feeling of thy speech: I will, out of thine own\n",
            "confession, learn to begin thy health; but, whilst I\n",
            "live, forget to drink after thee.\n",
            "\n",
            "First Gentleman:\n",
            "I think I have done myself wrong, have I not?\n",
            "\n",
            "Second Gentleman:\n",
            "Yes, that thou hast, whether thou art tainted or free.\n",
            "\n",
            "LUCIO:\n",
            "Behold, behold. where Madam Mitigation comes! I\n",
            "have purchased as many diseases under her roof as come to--\n",
            "\n",
            "Second Gentleman:\n",
            "To what, I pray?\n",
            "\n",
            "LUCIO:\n",
            "Judge.\n",
            "\n",
            "Second Gentleman:\n",
            "To three thousand dolours a year.\n",
            "\n",
            "First Gentleman:\n",
            "Ay, and more.\n",
            "\n",
            "LUCIO:\n",
            "A French crown more.\n",
            "\n",
            "First Gentleman:\n",
            "Thou art always figuring diseases in me; but thou\n",
            "art full of error; I am sound.\n",
            "\n",
            "LUCIO:\n",
            "Nay, not as one would say, healthy; but so sound as\n",
            "things that are hollow: thy bones are hollow;\n",
            "impiety has made a feast of thee.\n",
            "\n",
            "First Gentleman:\n",
            "How now! which of your hips has the most profound sciatica?\n",
            "\n",
            "MISTRESS OVERDONE:\n",
            "Well, well; there's one yonder arrested and carried\n",
            "to prison was worth five thousand of you all.\n",
            "\n",
            "Second Gentleman:\n",
            "Who's that, I pray thee?\n",
            "\n",
            "MISTRESS OVERDONE:\n",
            "Marry, sir, that's Claudio, Signior Claudio.\n",
            "\n",
            "First Gentleman:\n",
            "Claudio to prison? 'tis not so.\n",
            "\n",
            "MISTRESS OVERDONE:\n",
            "Nay, but I know\n",
            "---------------\n",
            "\n",
            "LEONTES:\n",
            "What! lack I am satisfied and that she's seen,\n",
            "Say you have said nothing she's not well.\n",
            "She had not so?\n",
            "\n",
            "POLIXENES:\n",
            "How, my wife?\n",
            "\n",
            "FLORIZEL:\n",
            "He neither does nor shall.\n",
            "\n",
            "POLIXENES:\n",
            "Methinks a father\n",
            "Is at the nuptial of his son a guest\n",
            "That best becomes the table. Pray you once more,\n",
            "Is not your father grown incapable\n",
            "Of reasonable affairs? is he not stupid\n",
            "With age and altering rheums? can he speak?\n",
            "Know man? dispute his own estate?\n",
            "Lies he not bed-rid? and again does nothing\n",
            "But what he did being childish?\n",
            "\n",
            "FLORIZEL:\n",
            "No, good sir;\n",
            "He has his health and ampler strength indeed\n",
            "Than most have of his age.\n",
            "\n",
            "POLIXENES:\n",
            "By my white beard,\n",
            "You offer him, if this be so, a wrong\n",
            "Something unfilial: reason my son\n",
            "Should choose himself a wife, but as good reason\n",
            "The father, all whose joy is nothing else\n",
            "But fair posterity, should hold some counsel\n",
            "In such a business.\n",
            "\n",
            "FLORIZEL:\n",
            "I yield all this;\n",
            "But for some other reasons, my grave sir,\n",
            "Which 'tis not fit you know, I not acquaint\n",
            "My father of this business.\n",
            "\n",
            "POLIXENES:\n",
            "Let him know't.\n",
            "\n",
            "FLORIZEL:\n",
            "He shall not.\n",
            "\n",
            "POLIXENES:\n",
            "Prithee, let him.\n",
            "\n",
            "FLORIZEL:\n",
            "No, he must not.\n",
            "\n",
            "Shepherd:\n",
            "Let him, my son: he shall not need to grieve\n",
            "At knowing of thy choice.\n",
            "\n",
            "FLORIZEL:\n",
            "Come, come, he must not.\n",
            "Mark our contract.\n",
            "\n",
            "POLIXENES:\n",
            "Mark your divorce, young sir,\n",
            "Whom son I dare not call; thou art too base\n",
            "To be acknowledged: thou a sceptre's heir,\n",
            "That thus affect'st a sheep-hook! Thou old traitor,\n",
            "I am sorry that by hanging thee I can\n",
            "But shorten thy life one week. And thou, fresh piece\n",
            "Of excellent witchcraft, who of force must know\n",
            "The royal fool thou copest with,--\n",
            "\n",
            "Shepherd:\n",
            "O,\n",
            "---------------\n",
            "\n",
            "Not to be so much: but it must I.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Take way, unruly woman!\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "After, Aumerle! mount thee upon his horse;\n",
            "Spur post, and get before him to the king,\n",
            "And beg thy pardon ere he do accuse thee.\n",
            "I'll not be long behind; though I be old,\n",
            "I doubt not but to ride as fast as York:\n",
            "And never will I rise up from the ground\n",
            "Till Bolingbroke have pardon'd thee. Away, be gone!\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Can no man tell me of my unthrifty son?\n",
            "'Tis full three months since I did see him last;\n",
            "If any plague hang over us, 'tis he.\n",
            "I would to God, my lords, he might be found:\n",
            "Inquire at London,'mongst the taverns there,\n",
            "For there, they say, they say, he daily doth frequent,\n",
            "With unrestrained loose companions,\n",
            "Even such, they say, as stand in narrow lanes,\n",
            "And beat our watch, and rob our passengers;\n",
            "Which he, young wanton and effeminate boy,\n",
            "Takes on the point of honour to support\n",
            "So dissolute a crew.\n",
            "\n",
            "HENRY PERCY:\n",
            "My lord, some two days since I saw the prince,\n",
            "And told him of those triumphs held at Oxford.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "And what said the gallant?\n",
            "\n",
            "HENRY PERCY:\n",
            "His answer was, he would unto the stews,\n",
            "And from the common'st creature pluck a glove,\n",
            "And wear it as a favour; and with that\n",
            "He would unhorse the lustiest challenger.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "As dissolute as desperate; yet through both\n",
            "I see some sparks of better hope, which elder years\n",
            "May happily bring forth. But who comes here?\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "What means our cousin, that he stares and looks\n",
            "So wildly?\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "God save your grace! I do beseech your majesty,\n",
            "To have some conference with your grace alone.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Withdraw yourselves, and leave us here alone.\n",
            "What is the matter with our cousin now?\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "For ever may\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2CTLNh-NA8wF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}